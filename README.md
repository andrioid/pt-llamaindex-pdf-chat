# andrllama1

### Install

To install dependencies:

```bash
bun install
```

To run:

```bash
bun run index.ts
```

### Setup LLM stuff

If you want to use ollama you can keep as is. Maybe adjust the model. If you want to use OpenAI, just uncomment the llm settings for that and remove the ollama stuff.

### Final words

I'm moving on, so don't expect updates. But, this could be expanded to create a chatbot that only responds to questions about files in the storage.
